"""
LLMå¤„ç†æ¨¡å— - è´Ÿè´£ä¸å„ç§LLM APIäº¤äº’
"""
import base64
import time
from pathlib import Path
from typing import Optional, Dict, Any
import openai
from anthropic import Anthropic
import google.generativeai as genai
from PIL import Image

class LLMHandler:
    """LLMå¤„ç†å™¨åŸºç±»"""
    
    def __init__(self, provider: str, api_key: str, model: str, prompt_template: str, 
                 max_retries: int = 3, timeout: int = 60):
        """
        åˆå§‹åŒ–LLMå¤„ç†å™¨
        
        Args:
            provider: LLMæä¾›å•† (openai, claude, gemini)
            api_key: APIå¯†é’¥
            model: æ¨¡å‹åç§°
            prompt_template: æç¤ºè¯æ¨¡æ¿
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            timeout: è¶…æ—¶æ—¶é—´(ç§’)
        """
        self.provider = provider
        self.api_key = api_key
        self.model = model
        self.prompt_template = prompt_template
        self.max_retries = max_retries
        self.timeout = timeout
        self.previous_summaries = []  # å­˜å‚¨å‰é¢é¡µé¢çš„æ‘˜è¦
        
    def analyze_image(self, image_path: str, page_num: int, 
                     additional_context: str = "", 
                     previous_context: str = "") -> str:
        """
        åˆ†æå›¾åƒå¹¶è¿”å›è®²è§£å†…å®¹
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            page_num: é¡µç 
            additional_context: é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            previous_context: å‰é¢é¡µé¢çš„ä¸Šä¸‹æ–‡æ‘˜è¦
            
        Returns:
            LLMç”Ÿæˆçš„è®²è§£å†…å®¹
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°æ­¤æ–¹æ³•")
    
    def extract_summary(self, analysis_text: str, page_num: int) -> str:
        """
        ä»åˆ†æç»“æœä¸­æå–å…³é”®æ‘˜è¦
        
        Args:
            analysis_text: å®Œæ•´çš„åˆ†ææ–‡æœ¬
            page_num: é¡µç 
            
        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        # ç®€å•æå–å‰200ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦,æˆ–è€…å¯ä»¥è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦
        lines = analysis_text.split('\n')
        summary_lines = []
        char_count = 0
        
        for line in lines:
            if char_count > 200:
                break
            if line.strip() and not line.startswith('#'):
                summary_lines.append(line.strip())
                char_count += len(line)
        
        summary = ' '.join(summary_lines)[:200]
        return f"[ç¬¬{page_num}é¡µæ‘˜è¦] {summary}"
    
    def add_to_context(self, summary: str, max_context_pages: int = 3):
        """
        æ·»åŠ æ‘˜è¦åˆ°ä¸Šä¸‹æ–‡å†å²
        
        Args:
            summary: é¡µé¢æ‘˜è¦
            max_context_pages: æœ€å¤§ä¿ç•™çš„ä¸Šä¸‹æ–‡é¡µæ•°
        """
        self.previous_summaries.append(summary)
        # åªä¿ç•™æœ€è¿‘Né¡µçš„æ‘˜è¦
        if len(self.previous_summaries) > max_context_pages:
            self.previous_summaries.pop(0)
    
    def get_context_string(self) -> str:
        """è·å–ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        if not self.previous_summaries:
            return ""
        
        context = "\n".join(self.previous_summaries)
        return f"\n\nğŸ“š å‰é¢é¡µé¢çš„å†…å®¹æ¦‚è¦:\n{context}\n"
    
    @staticmethod
    def encode_image_to_base64(image_path: str) -> str:
        """å°†å›¾åƒç¼–ç ä¸ºbase64"""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')


class OpenAIHandler(LLMHandler):
    """OpenAIå¤„ç†å™¨"""
    
    def __init__(self, api_key: str, model: str, prompt_template: str, 
                 max_retries: int = 3, timeout: int = 60):
        super().__init__("openai", api_key, model, prompt_template, max_retries, timeout)
        self.client = openai.OpenAI(api_key=api_key, timeout=timeout)
        
    def analyze_image(self, image_path: str, page_num: int, 
                     additional_context: str = "",
                     previous_context: str = "") -> str:
        """ä½¿ç”¨OpenAI Vision APIåˆ†æå›¾åƒ,æ”¯æŒé‡è¯•"""
        
        # ç¼–ç å›¾åƒ
        base64_image = self.encode_image_to_base64(image_path)
        
        # æ„å»ºæç¤ºè¯
        prompt = f"ã€ç¬¬ {page_num} é¡µã€‘\n\n{self.prompt_template}"
        
        # æ·»åŠ å‰é¢é¡µé¢çš„ä¸Šä¸‹æ–‡
        if previous_context:
            prompt += previous_context
        
        if additional_context:
            prompt += f"\n\nğŸ“„ å½“å‰é¡µé¢æå–çš„æ–‡æœ¬:\n{additional_context}"
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": prompt
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{base64_image}",
                                        "detail": "high"
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=2000,
                    temperature=0.7
                )
                
                return response.choices[0].message.content
                
            except Exception as e:
                error_msg = str(e)
                if attempt < self.max_retries - 1:
                    # å¦‚æœæ˜¯è¶…æ—¶æˆ–é€Ÿç‡é™åˆ¶é”™è¯¯,ç­‰å¾…åé‡è¯•
                    if "timeout" in error_msg.lower() or "504" in error_msg or "429" in error_msg:
                        wait_time = (attempt + 1) * 5  # é€’å¢ç­‰å¾…æ—¶é—´
                        print(f"  âš ï¸ ç¬¬{page_num}é¡µè¯·æ±‚å¤±è´¥,ç­‰å¾…{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{self.max_retries})")
                        time.sleep(wait_time)
                        continue
                
                return f"âŒ åˆ†æç¬¬ {page_num} é¡µæ—¶å‡ºé”™ (å°è¯•{attempt + 1}æ¬¡): {error_msg}"
        
        return f"âŒ åˆ†æç¬¬ {page_num} é¡µå¤±è´¥: å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°"


class ClaudeHandler(LLMHandler):
    """Anthropic Claudeå¤„ç†å™¨"""
    
    def __init__(self, api_key: str, model: str, prompt_template: str,
                 max_retries: int = 3, timeout: int = 60):
        super().__init__("claude", api_key, model, prompt_template, max_retries, timeout)
        self.client = Anthropic(api_key=api_key, timeout=timeout)
        
    def analyze_image(self, image_path: str, page_num: int, 
                     additional_context: str = "",
                     previous_context: str = "") -> str:
        """ä½¿ç”¨Claude Vision APIåˆ†æå›¾åƒ,æ”¯æŒé‡è¯•"""
        
        # è¯»å–å¹¶ç¼–ç å›¾åƒ
        base64_image = self.encode_image_to_base64(image_path)
        
        # è·å–å›¾åƒç±»å‹
        image_type = Path(image_path).suffix[1:]  # å»æ‰ç‚¹å·
        if image_type == 'jpg':
            image_type = 'jpeg'
        
        # æ„å»ºæç¤ºè¯
        prompt = f"ã€ç¬¬ {page_num} é¡µã€‘\n\n{self.prompt_template}"
        
        # æ·»åŠ å‰é¢é¡µé¢çš„ä¸Šä¸‹æ–‡
        if previous_context:
            prompt += previous_context
            
        if additional_context:
            prompt += f"\n\nğŸ“„ å½“å‰é¡µé¢æå–çš„æ–‡æœ¬:\n{additional_context}"
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                message = self.client.messages.create(
                    model=self.model,
                    max_tokens=2000,
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "image",
                                    "source": {
                                        "type": "base64",
                                        "media_type": f"image/{image_type}",
                                        "data": base64_image,
                                    },
                                },
                                {
                                    "type": "text",
                                    "text": prompt
                                }
                            ],
                        }
                    ],
                )
                
                return message.content[0].text
                
            except Exception as e:
                error_msg = str(e)
                if attempt < self.max_retries - 1:
                    if "timeout" in error_msg.lower() or "504" in error_msg or "429" in error_msg:
                        wait_time = (attempt + 1) * 5
                        print(f"  âš ï¸ ç¬¬{page_num}é¡µè¯·æ±‚å¤±è´¥,ç­‰å¾…{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{self.max_retries})")
                        time.sleep(wait_time)
                        continue
                
                return f"âŒ åˆ†æç¬¬ {page_num} é¡µæ—¶å‡ºé”™ (å°è¯•{attempt + 1}æ¬¡): {error_msg}"
        
        return f"âŒ åˆ†æç¬¬ {page_num} é¡µå¤±è´¥: å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°"


class GeminiHandler(LLMHandler):
    """Google Geminiå¤„ç†å™¨"""
    
    def __init__(self, api_key: str, model: str, prompt_template: str,
                 max_retries: int = 3, timeout: int = 60):
        super().__init__("gemini", api_key, model, prompt_template, max_retries, timeout)
        genai.configure(api_key=api_key)
        self.model_instance = genai.GenerativeModel(model)
        
    def analyze_image(self, image_path: str, page_num: int, 
                     additional_context: str = "",
                     previous_context: str = "") -> str:
        """ä½¿ç”¨Gemini Vision APIåˆ†æå›¾åƒ,æ”¯æŒé‡è¯•"""
        
        # åŠ è½½å›¾åƒ
        img = Image.open(image_path)
        
        # æ„å»ºæç¤ºè¯
        prompt = f"ã€ç¬¬ {page_num} é¡µã€‘\n\n{self.prompt_template}"
        
        # æ·»åŠ å‰é¢é¡µé¢çš„ä¸Šä¸‹æ–‡
        if previous_context:
            prompt += previous_context
            
        if additional_context:
            prompt += f"\n\nğŸ“„ å½“å‰é¡µé¢æå–çš„æ–‡æœ¬:\n{additional_context}"
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                response = self.model_instance.generate_content([prompt, img])
                return response.text
                
            except Exception as e:
                error_msg = str(e)
                if attempt < self.max_retries - 1:
                    if "timeout" in error_msg.lower() or "504" in error_msg or "429" in error_msg:
                        wait_time = (attempt + 1) * 5
                        print(f"  âš ï¸ ç¬¬{page_num}é¡µè¯·æ±‚å¤±è´¥,ç­‰å¾…{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{self.max_retries})")
                        time.sleep(wait_time)
                        continue
                
                return f"âŒ åˆ†æç¬¬ {page_num} é¡µæ—¶å‡ºé”™ (å°è¯•{attempt + 1}æ¬¡): {error_msg}"
        
        return f"âŒ åˆ†æç¬¬ {page_num} é¡µå¤±è´¥: å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°"


def create_llm_handler(provider: str, config: Any, prompt_template: str,
                      max_retries: int = 3, timeout: int = 60) -> LLMHandler:
    """
    å·¥å‚å‡½æ•°:åˆ›å»ºLLMå¤„ç†å™¨
    
    Args:
        provider: LLMæä¾›å•†
        config: é…ç½®å¯¹è±¡
        prompt_template: æç¤ºè¯æ¨¡æ¿
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        timeout: è¶…æ—¶æ—¶é—´(ç§’)
        
    Returns:
        LLMHandlerå®ä¾‹
    """
    if provider == 'openai':
        return OpenAIHandler(
            api_key=config.OPENAI_API_KEY,
            model=config.OPENAI_MODEL,
            prompt_template=prompt_template,
            max_retries=max_retries,
            timeout=timeout
        )
    elif provider == 'claude':
        return ClaudeHandler(
            api_key=config.ANTHROPIC_API_KEY,
            model=config.ANTHROPIC_MODEL,
            prompt_template=prompt_template,
            max_retries=max_retries,
            timeout=timeout
        )
    elif provider == 'gemini':
        return GeminiHandler(
            api_key=config.GOOGLE_API_KEY,
            model=config.GOOGLE_MODEL,
            prompt_template=prompt_template,
            max_retries=max_retries,
            timeout=timeout
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
